"""
AI Handler Module
Integrates with local Ollama DeepSeek model for question answering
"""
import json
import ollama
from typing import List, Dict, Any


class AIHandler:
    """Handles AI model interactions using Ollama with DeepSeek"""
    
    def __init__(self, model_name: str = "deepseek-r1:1.5b"):
        """
        Initialize AI handler
        
        Args:
            model_name: Ollama model name (default: deepseek-r1:1.5b)
        """
        self.model_name = model_name
        self.client = ollama.Client()
        
    def verify_model(self) -> bool:
        """
        Verify that the model is available in Ollama
        
        Returns:
            True if model is available, False otherwise
        """
        try:
            models = self.client.list()
            available_models = [model['name'] for model in models.get('models', [])]
            return any(self.model_name in model for model in available_models)
        except Exception:
            return False
    
    def answer_questions(self, context: str, questions: List[str]) -> Dict[str, Any]:
        """
        Answer multiple questions based on PDF context using DeepSeek
        """
        # Use batch processing for speed
        return self.batch_answer_with_context(context, questions)
    
    def _get_single_answer(self, context: str, question: str) -> str:
        """
        Get answer for a single question
        
        Args:
            context: PDF text content
            question: Question to answer
            
        Returns:
            Answer string
        """
        # Create prompt for DeepSeek
        prompt = f"""Based on the following document content, please answer the question accurately and concisely.

DOCUMENT CONTENT:
{context}

QUESTION: {question}

Please provide a clear, direct answer based only on the information in the document. If the answer cannot be found in the document, say "The information is not available in the provided document."

ANSWER:"""
        
        # Call Ollama DeepSeek model
        response = self.client.generate(
            model=self.model_name,
            prompt=prompt,
            options={
                "temperature": 0.3,  # Lower temperature for more focused answers
                "top_p": 0.9,
                "num_predict": 500,  # Limit response length
            }
        )
        
        answer = response['response'].strip()
        
        # Clean up the answer
        if not answer:
            answer = "No answer generated by the model."
        
        return answer
    
    def batch_answer_with_context(self, context: str, questions: List[str]) -> Dict[str, Any]:
        """
        Alternative method: Answer all questions in one batch call
        This can be more efficient but may be less accurate
        
        Args:
            context: Extracted text from PDF
            questions: List of questions
            
        Returns:
            Structured JSON response
        """
        # Truncate context if needed
        if len(context) > 12000:
            context = context[:12000] + "\n\n[Content truncated...]"
        
        # Format questions
        questions_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
        
        prompt = f"""Based on the following document, answer each question concisely and accurately.

DOCUMENT:
{context}

QUESTIONS:
{questions_text}

Provide answers in the following JSON format:
{{
  "answers": [
    "Answer to question 1...",
    "Answer to question 2..."
  ]
}}

Only include the JSON response, nothing else. The "answers" array must contain exactly {len(questions)} strings corresponding to the questions in order."""
        
        try:
            response = self.client.generate(
                model=self.model_name,
                prompt=prompt,
                options={
                    "temperature": 0.3,
                    "top_p": 0.9,
                }
            )
            
            # Try to parse JSON response
            answer_text = response['response'].strip()
            
            # Extract JSON if wrapped in markdown code blocks
            if "```json" in answer_text:
                answer_text = answer_text.split("```json")[1].split("```")[0].strip()
            elif "```" in answer_text:
                answer_text = answer_text.split("```")[1].split("```")[0].strip()
            
            parsed = json.loads(answer_text)
            
            # Format to our standard structure
            answers = []
            if 'answers' in parsed and isinstance(parsed['answers'], list):
                # Ensure we have strings
                for ans in parsed['answers']:
                    if isinstance(ans, str):
                        answers.append(ans)
                    elif isinstance(ans, dict) and 'answer' in ans:
                        answers.append(ans['answer'])
                    else:
                        answers.append(str(ans))
            
            # Ensure we have the same number of answers as questions
            while len(answers) < len(questions):
                answers.append("No answer generated.")
                
            return {
                "answers": answers[:len(questions)]
            }
            
        except Exception as e:
            # Fallback to individual question processing (using loop, not recursion)
            return self._answer_questions_sequentially(context, questions)

    def _answer_questions_sequentially(self, context: str, questions: List[str]) -> Dict[str, Any]:
        """
        Fallback: Answer questions one by one
        """
        answers = []
        for q in questions:
            try:
                ans = self._get_single_answer(context, q)
                answers.append(ans)
            except Exception:
                answers.append("Error generating answer.")
        
        return {
            "answers": answers
        }
